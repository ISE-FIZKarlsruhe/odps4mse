{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_metrics(eval_path):\n",
    "    \"\"\"Extracts precision, recall, and F1 from an evaluation file.\"\"\"\n",
    "    precision = recall = f1 = 0.0\n",
    "    extracted = 0\n",
    "    count_ground_truth = 0\n",
    "\n",
    "    if not os.path.exists(eval_path):\n",
    "        return (0.0, 0.0, 0.0, 0, 0)\n",
    "\n",
    "    with open(eval_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Precision:\"):\n",
    "                precision = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"Recall:\"):\n",
    "                recall = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"F1-score:\"):\n",
    "                f1 = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"GroundTruth:\"):\n",
    "                extracted = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"Predicted:\"):\n",
    "                count_ground_truth = float(line.split(\":\")[1])\n",
    "\n",
    "    return (precision, recall, f1, extracted, count_ground_truth)\n",
    "\n",
    "def count_ground_truth(gt_path):\n",
    "    \"\"\"Counts non-empty lines (ground truth IRIs).\"\"\"\n",
    "    if not os.path.exists(gt_path):\n",
    "        return 0\n",
    "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for line in f if line.strip())\n",
    "\n",
    "def generate_latex_table(ontology_dir, eval_dir, gt_dir, requirements, caption, label):\n",
    "    header = r\"\"\"\\begin{table}[ht]\n",
    "\\centering\n",
    "\\caption{%s}\n",
    "\\label{tab:%s}\n",
    "\\resizebox{\\textwidth}{!}{\n",
    "\\begin{tabular}{l\"\"\" % (caption, label)\n",
    "\n",
    "    # Each requirement gets 5 columns: P, R, F1, GT, Ext\n",
    "    header += \"ccccc|\" * len(requirements) + \"}\\n\"\n",
    "\n",
    "    # Subheaders: One block for each requirement\n",
    "    subheaders = [\"\\\\textbf{Ontology}\"]\n",
    "    for req in requirements:\n",
    "        subheaders += [f\"\\\\multicolumn{{5}}{{c}}{{{req}}}\"]\n",
    "\n",
    "    # Metrics headers per requirement\n",
    "    metrics_headers = [\"\"] + [\"P & R & $F_1$ & GT & Ext\"] * len(requirements)\n",
    "\n",
    "    table = header\n",
    "    table += \" & \".join(subheaders) + r\" \\\\\" + \"\\n\"\n",
    "    table += \" & \".join(metrics_headers) + r\" \\\\\" + \"\\n\"\n",
    "    table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    ontologies = sorted(os.listdir(ontology_dir))\n",
    "\n",
    "    for ontology_file in ontologies:\n",
    "        if not ontology_file.endswith(('.ttl', '.owl')):\n",
    "            continue\n",
    "        ontology_name = Path(ontology_file).stem\n",
    "        row = [ontology_name]\n",
    "\n",
    "        for req_id in requirements:\n",
    "            eval_file = os.path.join(eval_dir, ontology_name, f\"{req_id}_evaluation.txt\")\n",
    "            gt_file = os.path.join(gt_dir, ontology_name, f\"{req_id}.txt\")\n",
    "\n",
    "            P, R, F1, extracted, gt_count = extract_metrics(eval_file)\n",
    "\n",
    "            row += [f\"{P:.2f}\", f\"{R:.2f}\", f\"{F1:.2f}\", str(extracted), str(gt_count)]\n",
    "\n",
    "        table += \" & \".join(row) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "    table += r\"\\bottomrule\" + \"\\n\"\n",
    "    table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    table += r\"}\" + \"\\n\"\n",
    "    table += r\"\\end{table}\"\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table_multi_model(extracted_terms_dir, gt_dir, requirements, caption, label):\n",
    "    from collections import defaultdict\n",
    "    import os\n",
    "\n",
    "    # Table structure: dict[model][ontology][requirement] = (P, R, F1, GT, Extracted)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    # Traverse model folders in ExtractedTerms\n",
    "    for model_name in sorted(os.listdir(extracted_terms_dir)):\n",
    "        model_path = os.path.join(extracted_terms_dir, model_name)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "\n",
    "        for ontology_name in sorted(os.listdir(model_path)):\n",
    "            ontology_path = os.path.join(model_path, ontology_name)\n",
    "            if not os.path.isdir(ontology_path):\n",
    "                continue\n",
    "\n",
    "            for req_id in requirements:\n",
    "                eval_path = os.path.join(ontology_path, f\"{req_id}_evaluation.txt\")\n",
    "                gt_path = os.path.join(gt_dir, ontology_name, f\"{req_id}.txt\")\n",
    "\n",
    "                P, R, F1, extracted, gt_count = extract_metrics(eval_path)\n",
    "                results[model_name][ontology_name][req_id] = (P, R, F1, gt_count, extracted)\n",
    "\n",
    "    # Generate LaTeX table\n",
    "    models = sorted(results.keys())\n",
    "    ontologies = sorted({o for m in models for o in results[m].keys()})\n",
    "\n",
    "    table = r\"\"\"\\begin{table}[ht]\n",
    "\\centering\n",
    "\\caption{%s}\n",
    "\\label{tab:%s}\n",
    "\\resizebox{\\textwidth}{!}{\n",
    "\\begin{tabular}{l\"\"\" % (caption, label)\n",
    "\n",
    "    # For each model and each requirement: P, R, F1\n",
    "    for model in models:\n",
    "        table += \"ccc\" * len(requirements)\n",
    "    table += \"}\\n\"\n",
    "\n",
    "    # Top-level headers: model per group of 3 metrics\n",
    "    top_headers = [\"\\\\textbf{Ontology}\"]\n",
    "    for model in models:\n",
    "        for req in requirements:\n",
    "            safe_model = model.replace('_', r'\\_')\n",
    "            top_headers.append(f\"\\\\multicolumn{{3}}{{c}}{{{safe_model} - {req}}}\")\n",
    "\n",
    "\n",
    "    mid_headers = [\"\"] + [\"P & R & $F_1$\"] * len(models) * len(requirements)\n",
    "\n",
    "    table += \" & \".join(top_headers) + r\" \\\\\" + \"\\n\"\n",
    "    table += \" & \".join(mid_headers) + r\" \\\\\" + \"\\n\"\n",
    "    table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    for ontology in ontologies:\n",
    "        row = [ontology]\n",
    "        for model in models:\n",
    "            for req in requirements:\n",
    "                if req in results[model][ontology]:\n",
    "                    P, R, F1, _, _ = results[model][ontology][req]\n",
    "                    row += [f\"{P:.2f}\", f\"{R:.2f}\", f\"{F1:.2f}\"]\n",
    "                else:\n",
    "                    row += [\"--\", \"--\", \"--\"]\n",
    "        table += \" & \".join(row) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "    table += r\"\\bottomrule\" + \"\\n\"\n",
    "    table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    table += r\"}\" + \"\\n\"\n",
    "    table += r\"\\end{table}\" + \"\\n\"\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\"requirement1\", \"requirement2\", \"requirement3\", \"requirement4\"]\n",
    "\n",
    "latex_code = generate_latex_table(\n",
    "    ontology_dir=\"Ontologies\",\n",
    "    eval_dir=\"ExtractedTerms\",\n",
    "    gt_dir=\"GroundtruthTerms\",\n",
    "    requirements=requirements,\n",
    "    caption=\"Evaluation of ontology matching across requirement groups.\",\n",
    "    label=\"ontology-eval\"\n",
    ")\n",
    "\n",
    "with open(\"Latex/ontology_evaluation_table.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[ht]\n",
      "\\centering\n",
      "\\caption{Comparison of model performance for ODP extraction across requirements.}\n",
      "\\label{tab:odp-metrics-multimodel}\n",
      "\\resizebox{\\textwidth}{!}{\n",
      "\\begin{tabular}{lcccccccccccccccccccccccccccccccccccc}\n",
      "\\textbf{Ontology} & \\multicolumn{3}{c}{all-MiniLM-L6-v2 - requirement1} & \\multicolumn{3}{c}{all-MiniLM-L6-v2 - requirement2} & \\multicolumn{3}{c}{all-MiniLM-L6-v2 - requirement3} & \\multicolumn{3}{c}{all-MiniLM-L6-v2 - requirement4} & \\multicolumn{3}{c}{all-distilroberta-v1 - requirement1} & \\multicolumn{3}{c}{all-distilroberta-v1 - requirement2} & \\multicolumn{3}{c}{all-distilroberta-v1 - requirement3} & \\multicolumn{3}{c}{all-distilroberta-v1 - requirement4} & \\multicolumn{3}{c}{multi-qa-mpnet-base-dot-v1 - requirement1} & \\multicolumn{3}{c}{multi-qa-mpnet-base-dot-v1 - requirement2} & \\multicolumn{3}{c}{multi-qa-mpnet-base-dot-v1 - requirement3} & \\multicolumn{3}{c}{multi-qa-mpnet-base-dot-v1 - requirement4} \\\\\n",
      " & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ \\\\\n",
      "\\midrule\n",
      "GPO & 0.19 & 0.30 & 0.23 & 0.15 & 0.60 & 0.24 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.20 & 0.10 & 0.13 & 0.11 & 0.40 & 0.17 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.05 & 0.10 & 0.07 & 0.05 & 0.20 & 0.08 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "OntoCAPE & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "P-Plan & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.12 & 0.22 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.12 & 0.22 & 1.00 & 0.50 & 0.67 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "WILD & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "m4i & 0.25 & 0.62 & 0.36 & 0.15 & 0.27 & 0.19 & 0.07 & 0.12 & 0.09 & 0.20 & 0.33 & 0.25 & 0.15 & 0.25 & 0.19 & 0.25 & 0.45 & 0.32 & 0.00 & 0.00 & 0.00 & 0.25 & 0.42 & 0.31 & 0.25 & 0.62 & 0.36 & 0.15 & 0.27 & 0.19 & 0.10 & 0.25 & 0.14 & 0.20 & 0.33 & 0.25 \\\\\n",
      "opmw & 0.57 & 0.31 & 0.40 & 0.25 & 0.60 & 0.35 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.17 & 0.20 & 0.18 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.53 & 0.62 & 0.57 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "pmdcore & 0.20 & 0.44 & 0.28 & 0.10 & 0.33 & 0.15 & 0.00 & 0.00 & 0.00 & 0.15 & 0.75 & 0.25 & 0.25 & 0.56 & 0.34 & 0.15 & 0.50 & 0.23 & 0.00 & 0.00 & 0.00 & 0.15 & 0.75 & 0.25 & 0.20 & 0.44 & 0.28 & 0.15 & 0.50 & 0.23 & 0.00 & 0.00 & 0.00 & 0.15 & 0.75 & 0.25 \\\\\n",
      "stato & 0.33 & 0.22 & 0.27 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 0.05 & 0.11 & 0.07 & 0.05 & 0.11 & 0.07 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requirements = [\"requirement1\", \"requirement2\", \"requirement3\", \"requirement4\"]\n",
    "latex_code = generate_latex_table_multi_model(\n",
    "    extracted_terms_dir=\"ExtractedTerms\",\n",
    "    gt_dir=\"GroundtruthTerms\",\n",
    "    requirements=requirements,\n",
    "    caption=\"Comparison of model performance for ODP extraction across requirements.\",\n",
    "    label=\"odp-metrics-multimodel\"\n",
    ")\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_code = generate_latex_table(\n",
    "    ontology_dir=\"ODPs\",\n",
    "    eval_dir=\"ODPs/ExtractedTerms\",\n",
    "    gt_dir=\"ODPs/GroundtruthTerms\",\n",
    "    requirements=requirements,\n",
    "    caption=\"Evaluation of ODPs matching across requirement groups.\",\n",
    "    label=\"ontology-eval\"\n",
    ")\n",
    "\n",
    "with open(\"Latex/ODPs_evaluation_table.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "372ad118f81acd4222add26895f13bc27a937b9d5021100e967f342fe93795e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
