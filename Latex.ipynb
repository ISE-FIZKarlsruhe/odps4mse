{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_metrics(eval_path):\n",
    "    \"\"\"Extracts precision, recall, and F1 from an evaluation file.\"\"\"\n",
    "    precision = recall = f1 = 0.0\n",
    "    extracted = 0\n",
    "\n",
    "    if not os.path.exists(eval_path):\n",
    "        return (0.0, 0.0, 0.0, 0)\n",
    "\n",
    "    with open(eval_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Precision:\"):\n",
    "                precision = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"Recall:\"):\n",
    "                recall = float(line.split(\":\")[1])\n",
    "            elif line.startswith(\"F1-score:\"):\n",
    "                f1 = float(line.split(\":\")[1])\n",
    "            elif re.match(r'^https?://', line):\n",
    "                extracted += 1\n",
    "\n",
    "    return (precision, recall, f1, extracted)\n",
    "\n",
    "def count_ground_truth(gt_path):\n",
    "    \"\"\"Counts non-empty lines (ground truth IRIs).\"\"\"\n",
    "    if not os.path.exists(gt_path):\n",
    "        return 0\n",
    "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for line in f if line.strip())\n",
    "\n",
    "def generate_latex_table(ontology_dir, eval_dir, gt_dir, requirements, caption, label):\n",
    "    header = r\"\"\"\\begin{table}[ht]\n",
    "\\centering\n",
    "\\caption{%s}\n",
    "\\label{tab:%s}\n",
    "\\resizebox{\\textwidth}{!}{\n",
    "\\begin{tabular}{l\"\"\" % (caption, label)\n",
    "\n",
    "    # Add 4 groups of 3 columns (P, R, F1) and 2 final columns\n",
    "    header += \"ccc\" * len(requirements) + \"rr\" + \"}\\n\"\n",
    "\n",
    "    subheaders = [\"\\\\textbf{Ontology}\"]\n",
    "    for req in requirements:\n",
    "        subheaders += [f\"\\\\multicolumn{{3}}{{c}}{{{req}}}\"]\n",
    "    subheaders += [\"\\\\textbf{GT}\", \"\\\\textbf{Ext}\"]\n",
    "\n",
    "    metrics_headers = [\"\"] + [\"P & R & $F_1$\"] * len(requirements) + [\"\", \"\"]\n",
    "    \n",
    "    table = header\n",
    "    table += \" & \".join(subheaders) + r\" \\\\\" + \"\\n\"\n",
    "    table += \" & \".join(metrics_headers) + r\" \\\\\" + \"\\n\"\n",
    "    table += r\"\\midrule\" + \"\\n\"\n",
    "\n",
    "    ontologies = sorted(os.listdir(ontology_dir))\n",
    "    \n",
    "    for ontology_file in ontologies:\n",
    "        if not ontology_file.endswith(('.ttl', '.owl')):\n",
    "            continue\n",
    "        ontology_name = Path(ontology_file).stem\n",
    "        row = [ontology_name]\n",
    "\n",
    "        total_gt = 0\n",
    "        total_extracted = 0\n",
    "\n",
    "        for req_id in requirements:\n",
    "            eval_file = os.path.join(eval_dir, ontology_name, f\"{req_id}_evaluation.txt\")\n",
    "            gt_file = os.path.join(gt_dir, ontology_name, f\"{req_id}.txt\")\n",
    "\n",
    "            P, R, F1, extracted = extract_metrics(eval_file)\n",
    "            gt_count = count_ground_truth(gt_file)\n",
    "\n",
    "            row += [f\"{P:.2f}\", f\"{R:.2f}\", f\"{F1:.2f}\"]\n",
    "            total_gt += gt_count\n",
    "            total_extracted += extracted\n",
    "\n",
    "        row += [str(total_gt), str(total_extracted)]\n",
    "        table += \" & \".join(row) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "    table += r\"\\bottomrule\" + \"\\n\"\n",
    "    table += r\"\\end{tabular}\" + \"\\n\"\n",
    "    table += r\"}\" + \"\\n\"\n",
    "    table += r\"\\end{table}\"\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\"requirement1\", \"requirement2\", \"requirement3\", \"requirement4\"]\n",
    "\n",
    "latex_code = generate_latex_table(\n",
    "    ontology_dir=\"Ontologies\",\n",
    "    eval_dir=\"ExtractedTerms\",\n",
    "    gt_dir=\"GroundtruthTerms\",\n",
    "    requirements=requirements,\n",
    "    caption=\"Evaluation of ontology matching across requirement groups.\",\n",
    "    label=\"ontology-eval\"\n",
    ")\n",
    "\n",
    "with open(\"Latex/ontology_evaluation_table.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_code = generate_latex_table(\n",
    "    ontology_dir=\"ODPs\",\n",
    "    eval_dir=\"ODPs/ExtractedTerms\",\n",
    "    gt_dir=\"ODPs/GroundtruthTerms\",\n",
    "    requirements=requirements,\n",
    "    caption=\"Evaluation of ODPs matching across requirement groups.\",\n",
    "    label=\"ontology-eval\"\n",
    ")\n",
    "\n",
    "with open(\"Latex/ODPs_evaluation_table.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "372ad118f81acd4222add26895f13bc27a937b9d5021100e967f342fe93795e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
